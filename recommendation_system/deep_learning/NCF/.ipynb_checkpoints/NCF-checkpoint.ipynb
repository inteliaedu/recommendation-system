{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF -  Construcción\n",
    "\n",
    "## Importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IL0mA-RBTvIQ",
    "outputId": "80fc9995-a4df-41b7-89fe-3cb66f3c2f25"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "\n",
    "## for machine learning\n",
    "from sklearn import metrics, preprocessing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, utils , regularizers, initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0rRPberLHXe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "971sLjQ0ZQkb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(engine=\"pyarrow\", path=\"/data/data_modelo_deep_learning.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EP7pvIPLKr0"
   },
   "outputs": [],
   "source": [
    "df_user_idx = 'user_idx'\n",
    "df_target = 'PURCHASE_PRED'\n",
    "df_item_idx = 'item_idx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZzsmvmIiJW-"
   },
   "outputs": [],
   "source": [
    "# Separación set de datos\n",
    "train, test = train_test_split(df, stratify=df[df_user_idx], test_size=0.2,random_state=42)\n",
    "train, val = train_test_split(train, stratify=train[df_user_idx], test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeZ6HAcNOOEU",
    "outputId": "23b49d54-11cf-4ad8-8f1b-a3ce5e40973e"
   },
   "outputs": [],
   "source": [
    "# Definición de dimensiones en modelo de Deep Learning\n",
    "usr, prd = len(df[df_user_idx].unique())+1, len(df[df_item_idx].unique())+1\n",
    "print(usr, prd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSSTNu-M_Q4C"
   },
   "outputs": [],
   "source": [
    "#Hiperparámetros en entrenamiento\n",
    "batch_size = 64\n",
    "EPOCHS = 10\n",
    "embeddings_size= 50\n",
    "initial_lr=0.00001\n",
    "max_lr= 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWaiKAMw1bQE"
   },
   "outputs": [],
   "source": [
    "# Definiendo learning rate cíclico\n",
    "steps_per_epoch = int(len(train) / batch_size)\n",
    "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=initial_lr,\n",
    "    maximal_learning_rate=max_lr,\n",
    "    scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "    step_size= 2 * steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xM_ougHHsoio"
   },
   "outputs": [],
   "source": [
    "# Guardado de resultados en clase para conservar propiedades de listas en objeto\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_epoch_error = []\n",
    "        self.val_epoch_error = []\n",
    "        self.train_epoch_loss = []\n",
    "        self.val_epoch_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        self.train_epoch_loss.append(logs.get(\"loss\"))\n",
    "        self.val_epoch_loss.append(logs.get(\"val_loss\"))\n",
    "        self.train_epoch_error.append(logs.get(\"root_mean_squared_error\"))\n",
    "        self.val_epoch_error.append(logs.get(\"val_root_mean_squared_error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z7dN3_JOW_q"
   },
   "outputs": [],
   "source": [
    "# Clientes (1,embedding_size)\n",
    "# Input layer\n",
    "xusers_in = layers.Input(name=\"xusers_in\", shape=(1,))\n",
    "xproducts_in = layers.Input(name=\"xproducts_in\", shape=(1,))\n",
    "\n",
    "# A) Matrix Factorization\n",
    "\n",
    "# Clientes (1,embedding_size)\n",
    "xusers_emb = layers.Embedding(name=\"xusers_emb\", input_dim=usr, output_dim=embeddings_size,embeddings_regularizer=keras.regularizers.l2(1e-6))(xusers_in)\n",
    "xusers = layers.Reshape(name='xusers', target_shape=(embeddings_size,))(xusers_emb)\n",
    "\n",
    "# Productos (1,embedding_size)\n",
    "xproducts_emb = layers.Embedding(name=\"xproducts_emb\", input_dim=prd, output_dim=embeddings_size,embeddings_regularizer=keras.regularizers.l2(1e-6))(xproducts_in)\n",
    "xproducts = layers.Reshape(name='xproducts', target_shape=(embeddings_size,))(xproducts_emb)\n",
    "\n",
    "# Productos (1)\n",
    "xx = layers.multiply([xusers, xproducts])\n",
    "\n",
    "# B) Red Neuronal\n",
    "\n",
    "## embeddings & reshape\n",
    "nn_xusers_emb = layers.Embedding(name=\"nn_xusers_emb\", input_dim=usr, output_dim=embeddings_size,embeddings_regularizer=keras.regularizers.l2(1e-6))(xusers_in)\n",
    "nn_xusers = layers.Reshape(name='nn_xusers', target_shape=(embeddings_size,))(nn_xusers_emb)\n",
    "## embeddings & reshape\n",
    "nn_xproducts_emb = layers.Embedding(name=\"nn_xproducts_emb\", input_dim=prd, output_dim=embeddings_size,embeddings_regularizer=keras.regularizers.l2(1e-6))(xproducts_in)\n",
    "nn_xproducts = layers.Reshape(name='nn_xproducts', target_shape=(embeddings_size,))(nn_xproducts_emb)\n",
    "## concat & dense\n",
    "nn_xx_1 = layers.Concatenate()([nn_xusers, nn_xproducts])\n",
    "nn_xx_2 = layers.Dense(name=\"nn_xx_1\", units=128, activation='relu',kernel_initializer=initializers.GlorotUniform(seed=1),activity_regularizer=keras.regularizers.L2(1e-6))(nn_xx_1)\n",
    "nn_xx_3 = layers.Dense(name=\"nn_xx_2\", units=64, activation='relu',kernel_initializer=initializers.GlorotUniform(seed=2),activity_regularizer=keras.regularizers.L2(1e-6))(nn_xx_2)\n",
    "nn_xx_4 = layers.Dense(name=\"nn_xx_3\", units=32, activation='relu',kernel_initializer=initializers.GlorotUniform(seed=3),activity_regularizer=keras.regularizers.L2(1e-6))(nn_xx_3)\n",
    "\n",
    "\n",
    "# C) Concatenación y salida\n",
    "nn_xx = layers.Concatenate()([nn_xx_4,xx])\n",
    "\n",
    "# Neurona de salida\n",
    "y_out = layers.Dense(name=\"y_out\", units=1, activation='linear')(nn_xx)\n",
    "\n",
    "# Compilando\n",
    "optimizer = Adam(learning_rate=clr)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "model = models.Model(inputs=[xusers_in,xproducts_in], outputs=y_out, name=\"NCF\")\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(delta=0.75), metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mhMRrABsvV3"
   },
   "outputs": [],
   "source": [
    "# Creando objeto CustomCallback\n",
    "callback_results = CustomCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVeOHzc6tvvt",
    "outputId": "98c1fa28-74bb-4911-ee17-e915fc60a4cc"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9lhbFToPaTf",
    "outputId": "bfefd6be-4f2e-4dbc-d5bb-97f2f7e363e6"
   },
   "outputs": [],
   "source": [
    "#Default epochs 100\n",
    "training = model.fit(x=[train[df_user_idx], train[df_item_idx]], y=train[df_target], epochs=EPOCHS, callbacks=[callback_results]\n",
    "                    ,batch_size=batch_size, shuffle=True, verbose=1, validation_data=([val[df_user_idx], val[df_item_idx]],val[df_target]))\n",
    "model = training.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUUpFz3BqYK2",
    "outputId": "e04035b6-724e-4ca2-d0b1-fb37560cf779"
   },
   "outputs": [],
   "source": [
    "# Evaluando en set de datos de prueba\n",
    "results = model.evaluate([test[df_user_idx], test[df_item_idx]], test[df_target], batch_size=batch_size)\n",
    "print(\"Prueba huber loss, Prueba rmse:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XG-yWsDpoe-"
   },
   "outputs": [],
   "source": [
    "# Resultados entrenamiento y validación\n",
    "df = pd.DataFrame(list(zip(callback_results.train_epoch_loss, callback_results.val_epoch_loss, callback_results.train_epoch_error, callback_results.val_epoch_error)),\n",
    "              columns=['train_loss','val_loss', 'train_error', 'val_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2v00cwfVqyf9"
   },
   "outputs": [],
   "source": [
    "# Resultados testing\n",
    "df_2 = pd.DataFrame(list(zip([results[0]],[results[1]])),\n",
    "              columns=['test_loss', 'test_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv2ehcKgyg8I"
   },
   "outputs": [],
   "source": [
    "# Obteniendo predicciones en Prueba\n",
    "predicciones = model.predict([test[df_user_idx], test[df_item_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yqv5dAcwygrw"
   },
   "outputs": [],
   "source": [
    "# Función para obtener resultados en ranking\n",
    "def compute_predictions(df_pred,k=6):\n",
    "  df_pred['rank_by_client'] = df_pred.groupby(df_user_idx)[df_target].rank(method='first',ascending=False)\n",
    "  df_pred['rank_by_model'] = df_pred.groupby(df_user_idx)[column_estimation].rank(method='first',ascending=False)\n",
    "  df_pred = df_pred.sort_values([df_user_idx, 'rank_by_client'],ascending = [True, True])\n",
    "  df_pred_clients_k = df_pred[df_pred[df_user_idx].isin(df_pred[df_pred['rank_by_client']==int(k*2)][df_user_idx].unique())]\n",
    "  df_pred_clients_k = df_pred_clients_k[df_pred_clients_k['rank_by_client']<=k].copy()\n",
    "  df_pred_clients_k['Discounted_Gain'] =    np.where(df_pred_clients_k['rank_by_model']<=k,((k+1 - df_pred_clients_k['rank_by_client'])/k)/ np.log2(df_pred_clients_k['rank_by_model'] + 1),0)\n",
    "  df_pred_clients_k['Ideal_Discounted_Gain'] =  ((k+1- df_pred_clients_k['rank_by_client']) / k )/ np.log2(df_pred_clients_k['rank_by_client'] + 1)\n",
    "  df_pred_clients_k['Precions_k'] = np.where(df_pred_clients_k['rank_by_model']<=k,1,0)\n",
    "  df_pred_clients_k['Accuracy'] = np.where(df_pred_clients_k['rank_by_model']==df_pred_clients_k['rank_by_client'],1,0)\n",
    "  df_pred_clients_k['Penalized_Ranking'] =  np.where(df_pred_clients_k['rank_by_client']<df_pred_clients_k['rank_by_model'],0,df_pred_clients_k['rank_by_model'] / df_pred_clients_k['rank_by_client'])\n",
    "  df_pred_clients_k['MRR'] = np.where(df_pred_clients_k['rank_by_model']==df_pred_clients_k['rank_by_client'], 1/df_pred_clients_k['rank_by_client'] ,0)\n",
    "  df_pred_clients_k['IMRR'] = 1/df_pred_clients_k['rank_by_client']\n",
    "  grouped = df_pred_clients_k.groupby(df_user_idx).agg({'MRR': 'sum', 'IMRR': 'sum',  'Ideal_Discounted_Gain': 'sum' ,'Discounted_Gain': 'sum' })\n",
    "  grouped['ndcgk'] =grouped['Discounted_Gain'] / grouped['Ideal_Discounted_Gain']\n",
    "  ndcgk_mean = grouped['ndcgk'].mean()\n",
    "  mrr_ratio =  grouped['MRR'].mean()/ grouped['IMRR'].mean()\n",
    "  return [df_pred_clients_k['Precions_k'].mean(),df_pred_clients_k['Accuracy'].mean(),df_pred_clients_k['Penalized_Ranking'].mean(),ndcgk_mean, mrr_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdDeUhL_ygCo"
   },
   "outputs": [],
   "source": [
    "# Agregando columna de predicciones en prueba\n",
    "column_estimation = 'Prediction'\n",
    "test[column_estimation] = predicciones\n",
    "(abs((test.PURCHASE_PRED - test.Prediction))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgQWhwxmywYd"
   },
   "outputs": [],
   "source": [
    "# Obteniendo métricas de rendimiento en ranking\n",
    "compute_predictions(test,k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2ZhkBNRtx0_"
   },
   "outputs": [],
   "source": [
    "# Gráfica en función de perdida en entrenamiento y validación por época\n",
    "sns.set_theme()\n",
    "plt.plot(range(1, len(callback_results.train_epoch_loss) + 1), callback_results.train_epoch_loss, label='Hybrid NCF Loss')\n",
    "plt.plot(range(1, len(callback_results.val_epoch_loss) + 1), callback_results.val_epoch_loss, label='Hybrid NCF Loss')\n",
    "plt.title('Comparación modelos en entrenamiento - Huber Loss')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Huber Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
